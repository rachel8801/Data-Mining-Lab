{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Initialization\n",
    "In the first task of the lab, we will model linear regression based on a data set that contains grades from CS 205 course in fall 2018. The dataset (with no ID's) contain midterm and final exam grades of Rutgers students (including other assignment grades). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.1  Read Data\n",
    "Read the file into a dataframe and keep only the midtermRaw and FinaRaw columns. We will be doing univariate regression on x=midterm, y=finalExam\n",
    "The goal is to find a model that will allow us to predict the final exam score given the midterm score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midtermRaw</th>\n",
       "      <th>finalRaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.5</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.0</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64.5</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   midtermRaw  finalRaw\n",
       "3        45.5      62.0\n",
       "4        58.0      60.5\n",
       "5        68.0      32.0\n",
       "6        64.5      50.5\n",
       "7        74.0      51.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/CS205_grades_12_19_18_Final.csv\")\n",
    "df_cleaned = df[['midtermRaw','finalRaw']]\n",
    "# drop all undefined rows \n",
    "df_cleaned = df_cleaned.dropna() \n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.2 Normalize Data\n",
    "In this task, you need to normalize data using MinMaxScaler from sklearn.preprocessing. Normalize midterm and final scores to be between 0 and 1. X_scaled_values and Y_scaled_values are the normalized midterm and final exam scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "df2=scaler.fit_transform(df_cleaned)\n",
    "X_scaled_values=df2[:,0]\n",
    "Y_scaled_values=df2[:,1]\n",
    "\n",
    "# call the scaled vectors x and y\n",
    "x = X_scaled_values\n",
    "y = Y_scaled_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.3 Plot the data to see if a linear regression line is a good fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1193a5940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4\n",
    "### BEGIN ANSWER\n",
    "question: Based on what you see in the plot, do you think it is fine to use linear regression? Why?\n",
    "- Yes, as we can see there is a linear relationship between x and y variables, on each x value we can find its y value. If we look at the most dense of the dots, it looks like a positive slop line go through it.\n",
    "\n",
    "### END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Manual Exploration of Linear Regression Line\n",
    "In this task we will manually explore the linear regression line. This will give us a good intution about the process.\n",
    "The goal now is to fit a line \n",
    "$$\n",
    "h(\\theta) = \\theta_0 + \\theta_1*x \n",
    "$$\n",
    "to all data points (x,y), such that the L2 error (or any other error such as L1 or Huber)\n",
    "$$\n",
    "E(\\theta) = \\sum(h(\\theta)-y)^2 $$ is minimized. In this task we will manually change the values of theta0 and theta1 such that we obtain the smallest possible error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function h(theta)\n",
    "def h(theta0, theta1, x):\n",
    "    \"\"\"\n",
    "    Return the model theta0 + theta1*x\n",
    "    \n",
    "    \"\"\"\n",
    "    return theta0 + theta1 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.1 - Define the square loss (L2) function\n",
    "Define the function, sqerror that computes the error based on the arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. We use the average square loss or L2 loss in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02348942348323907"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sqerror(x, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model\n",
    "    Input: x, y vectors\n",
    "    Returns: L2 square error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    sum0=0\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        sum0 += (theta1 * x[i] +theta0 - y[i])**2\n",
    "    return sum0/n\n",
    "\n",
    "## testing\n",
    "sqerror(x, y, 0.29,0.52)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.2 - Define the L1 Absolute error function\n",
    "Define the function, abserror that computes the avarega absolute error based on the arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. We use the average abssolute error in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1258878083450344"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def abserror(x, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: L1 error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    return np.mean(np.absolute(h(theta0, theta1, x)-y))\n",
    "## testing\n",
    "abserror(x, y, 0.29,0.52)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.3 - Define the Pseudo Huber loss function\n",
    "Define the function, huberror that computes the pseudo huber error based on the two arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. The equation for this function is given by \n",
    "![image.png](attachment:image.png)\n",
    "The following function finds the average huber error. In this equation, a  = |h(theta) - y| (see notes for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006994227909507368"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def huberror(x, y, theta0, theta1, delta):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0, theta1 and delta of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: psuedo huber error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(delta**2 *(np.sqrt(1 + ((np.abs(h(theta0, theta1, x)-y))/delta)**2) -1))\n",
    "\n",
    "## testing\n",
    "huberror(x, y, 0.29,0.52,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.4 Interactive Exploration.\n",
    "Let us initialize the interat widget (as in Lab5) to create sliders that allows us to change the values of theta0 and theta1 and see how things change. Complete the function f below. The function is expected to get two values theta0 and theta1 and plot both the observed points (x,y) and the regression line on the same plot. It also needs to compute the error and display and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93dfc370ff0845fa92fbdf392ef228b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='theta0', max=1.0), FloatSlider(value=0.0, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# interactive panel\n",
    "import pylab\n",
    "import numpy\n",
    "\n",
    "def f(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Plot the line and points in an interactive panel\n",
    "    \"\"\"\n",
    "    # plot the line for theta0 and theta1\n",
    "    y1 = h(theta0, theta1, x) \n",
    "    # compose plot\n",
    "    pylab.plot(x,y1) \n",
    "    \n",
    "    # compute the L2 error for theta0 and theta1 for 5 decimal places\n",
    "    sqerr = round(sqerror(x, y, theta0, theta1),6)\n",
    "    # compute the absolute or L1 error for theta0 and theta1\n",
    "    abserr = round(abserror(x, y, theta0, theta1),4)\n",
    "    # compute the phub error for theta0 and theta1\n",
    "    huberr = round(huberror(x, y, theta0, theta1, 0.01),4)\n",
    "    pylab.title('L1=' + str(abserr) + '  L2=' + str(sqerr) + '  hub=' + str(huberr))\n",
    "    \n",
    "    # plot the points\n",
    "    pylab.scatter(x, y, alpha=0.5)\n",
    "    pylab.show() # show the plot  \n",
    "\n",
    "interact(f, theta1=(0,1,0.1), theta0=(0,1,0.1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.5 Record the best values for each error function\n",
    "Write the \"best\" values you found for theta0 (y-intercept) and theta1 (slope) and the error. \n",
    "This error is the minimum you have observed based on the manual exploration using the widget \n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST VALUES FOR AVE SQUARE ERROR\n",
    "theta0 = 0.2\n",
    "theta1 =0.7\n",
    "error = 0.1288\n",
    "# BEST VALUES FOR AVE ABS ERROR\n",
    "theta0 =0.2\n",
    "theta1 =0.7\n",
    "error = 0.025128\n",
    "\n",
    "# BEST VALUES FOR AVE HUBER ERROR\n",
    "theta0 =0.2\n",
    "theta1 =0.7\n",
    "error = 0.0012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Gradient Descent\n",
    "In this task we use the Gradient descent methods to find a \"better\" values for theta0 and theta1 that minimizes the error. Gradient descent is an iterative algorithm. It computes values of theta0 and theta1 in the direction of reaching the minimum point in the error function. The iterative formulas using L2 loss function for theta0 and theta1 are given by:\n",
    "$$\n",
    "\\theta_0 = \\theta_0 - \\alpha*(\\sum(\\theta_1*x_j + \\theta_0)-y_j)\n",
    "$$\n",
    "$$\n",
    "\\theta_1 = \\theta_1 - \\alpha*(\\sum(\\theta_1*x_j + \\theta_0 - y_j)*x_j\n",
    "$$\n",
    "\n",
    "The alpha is called the \"learning rate\". It is important to pick a good value for alpha so that convergence is not too slow (small alpha) or be at the risk of over shooting the minimum point (large alpha). You may have to experiemnt with few alphas to find something that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.1 Compute using Gradient Descent algorithm (L2 loss)\n",
    "\n",
    "Please print out the theta0 and theta1 values for each iteration in your function. You may get different output compare with the sample output depends on your initial theta0 and theta1 values. We will accept any answers which are close to the sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05437313432835818 0.9967095948827293 0.23980749256935113 1\n",
      "0.044009367164179046 0.9593776206883948 0.20347169181668837 2\n",
      "0.07069614881245201 0.9441697657455459 0.20122272144465997 3\n",
      "0.07445850260114387 0.917853040028166 0.18177344667160272 4\n",
      "0.09072534967175905 0.8999270692939488 0.17392301537443877 5\n",
      "0.09865378179349293 0.8787952165048333 0.16115338989852493 6\n",
      "0.11058707857120836 0.8612063155101113 0.1526715530874735 7\n",
      "0.11929279485989783 0.8430938322678296 0.14311161078149046 8\n",
      "0.12908170670422459 0.8267745889439873 0.13539448072459512 9\n",
      "0.13745408438385084 0.8108047769006026 0.12774739816494188 10\n",
      "0.1459247851577802 0.7959626266385375 0.12107597240750992 11\n",
      "0.15363876380038088 0.7817196378809479 0.11477267452868725 12\n",
      "0.16113673439377352 0.7683216378319104 0.10909976282203313 13\n",
      "0.1681342991892166 0.7555611908467732 0.10383224658782939 14\n",
      "0.17483191027358902 0.7435014484390304 0.0990299024322874 15\n",
      "0.1811423520316239 0.7320490552541277 0.09459663726863012 16\n",
      "0.1871464583906476 0.7212058870404141 0.0905311185572727 17\n",
      "0.19282441643519968 0.7109204240254063 0.08678319251208755 18\n",
      "0.19821432250772275 0.7011752660068699 0.08333535604019049 19\n",
      "0.2033187419811097 0.6919353793312275 0.08015574217882607 20\n",
      "0.20815989172807386 0.6831784908992979 0.07722458599922737 21\n",
      "0.21274716165545826 0.6748770388764096 0.07451870574697375 22\n",
      "0.21709633483906016 0.6670087015151075 0.07202005050050209 23\n",
      "0.2212183126290394 0.6595500767728106 0.06971049734582652 24\n",
      "0.2251258224445794 0.6524803052964804 0.06757455969329773 25\n",
      "0.22882951641809623 0.6457788342997017 0.06559758516973992 26\n",
      "0.23234032442309024 0.6394266422973868 0.06376656672633951 27\n",
      "0.23566811557544864 0.6334054275423742 0.06206947471872419 28\n",
      "0.23882253441591944 0.6276980021134428 0.06049545888441835 29\n",
      "0.24181255203684543 0.6222879793208663 0.05903456775698814 30\n",
      "0.24464677282199976 0.6171598820883492 0.05767776188434691 31\n",
      "0.24733329333978296 0.6122990070040363 0.056416772256195145 32\n",
      "0.2498798233005805 0.6076914362304113 0.05524406097168802 33\n",
      "0.2522936499832269 0.6033239657651831 0.05415273465976586 34\n",
      "0.2545816931190818 0.5991840864103972 0.053136495740723926 35\n",
      "0.2567505046578008 0.5952599368509729 0.05218958135578474 36\n",
      "0.25880629936708066 0.5915402762130091 0.05130671801190507 37\n",
      "0.26075496565926076 0.588014448005086 0.05048307487903085 38\n",
      "0.2626020865754173 0.5846723518642843 0.04971422484864401 39\n",
      "0.26435295335236314 0.5815044132795762 0.0489961073937074 40\n",
      "0.26601258201231054 0.5785015569542504 0.048324996064289256 41\n",
      "0.2675857268883098 0.5756551803406743 0.04769746841924628 42\n",
      "0.269076894742268 0.5729571292712162 0.04711037916842947 43\n",
      "0.27049035738207483 0.5703996744361345 0.04656083562438197 44\n",
      "0.27183016407255745 0.5679754893366432 0.0460461755800288 45\n",
      "0.27310015303351526 0.5656776292397361 0.04556394717824256 46\n",
      "0.27430396249558153 0.5634995113164478 0.04511189069553477 47\n",
      "0.27544504108736695 0.561434895763942 0.044687921987616064 48\n",
      "0.27652665773593754 0.559477867941515 0.044290117475545975 49\n",
      "0.2775519110191663 0.5576228214196964 0.043916700501450606 50\n",
      "0.2785237380497276 0.5558644419231653 0.0435660289356123 51\n",
      "0.27944492288494027 0.5541976921042134 0.0432365839074248 52\n",
      "0.28031810450497896 0.5526177971133203 0.04292695955746629 53\n",
      "0.28114578437144155 0.551120230919495 0.0426358537109034 54\n",
      "0.2819303335943354 0.5497007033447406 0.04236205938638122 55\n",
      "0.2826739997242349 0.5483551477734063 0.04210445706052423 56\n",
      "0.28337891319129666 0.5470797095024513 0.04186200761739361 57\n",
      "0.2840470934082642 0.5458707346985189 0.04163374591830113 58\n",
      "0.28468045455572166 0.5447247599306146 0.04141877493415117 59\n",
      "0.28528081106571207 0.5436385022481494 0.041216260387808026 60\n",
      "0.2858498828196992 0.5426088497760713 0.041025425859236755 61\n",
      "0.286389300075605 0.541632852800057 0.04084554831063313 62\n",
      "0.28690060813813195 0.5407077153162754 0.040675953992937254 63\n",
      "0.28738527178569256 0.5398307870214831 0.04051601469880167 64\n",
      "0.2878446794666634 0.5389995557205274 0.04036514433045375 65\n",
      "0.2882801472769623 0.5382116401294897 0.04022279575389371 66\n",
      "0.28869292273035907 0.5374647830538645 0.04008845791360096 67\n",
      "0.28908418833230987 0.5367568449222213 0.03996165318436766 68\n",
      "0.28945506496756085 0.536085797656833 0.039841934939096456 69\n",
      "0.28980661511122097 0.5354497188637044 0.03972888531339279 70\n",
      "0.2901398458725058 0.5348467863253628 0.03962211314958774 71\n",
      "0.29045571187987074 0.5342752727806286 0.03952125210445086 72\n",
      "0.29075511801579873 0.5337335409764155 0.03942595890632373 73\n",
      "0.291038922009078 0.5332200389773815 0.03933591174872924 74\n",
      "0.2913079368919949 0.5327332957199962 0.03925080880871131 75\n",
      "0.29156293332948185 0.5322719167982866 0.03917036687924099 76\n",
      "0.2918046418268928 0.5318345804691889 0.03909432010600429 77\n",
      "0.2920337548227309 0.5314200338660628 0.03902241881977041 78\n",
      "0.29225092867232316 0.531027089409521 0.038954428456339646 79\n",
      "0.29245678552812565 0.5306546214052904 0.03889012855679377 80\n",
      "0.29265191512204525 0.5303015628193607 0.038829311841425684 81\n",
      "0.292836876454884 0.5299669022211808 0.03877178335131848 82\n",
      "0.2930121993977462 0.5296496808861461 0.03871735965208036 83\n",
      "0.2931783862099955 0.5293489900490754 0.03866586809472748 84\n",
      "0.2933359129781111 0.5290639683008116 0.038617146129148607 85\n",
      "0.2934852309795643 0.5287937991204841 0.03857104066598233 86\n",
      "0.2936267679756233 0.5285377085363673 0.038527407483103224 87\n",
      "0.2937609294367898 0.5282949629086322 0.038486110673237546 88\n",
      "0.293888099704377 0.5280648668276392 0.03844702212953168 89\n",
      "0.2940086430915584 0.5278467611217513 0.03841002106616347 90\n",
      "0.2941229049270397 0.5276400209689615 0.03837499357133535 91\n",
      "0.29423121254434514 0.5274440541069229 0.03834183219021018 92\n",
      "0.29433387621955204 0.5272582991362553 0.03831043553555464 93\n",
      "0.294431190060159 0.5270822239122673 0.03828070792404019 94\n",
      "0.2945234328476356 0.526915324020485 0.03825255903631906 95\n",
      "0.29461086883606624 0.5267571213316227 0.03822590359914758 96\n",
      "0.2946937485091766 0.5266071626318536 0.03820066108796748 97\n",
      "0.2947723092979109 0.5264650183244581 0.03817675544848426 98\n",
      "0.29484677626061645 0.5263302811991293 0.03815411483589725 99\n",
      "0.29491736272778324 0.5262025652654101 0.03813267137054281 100\n",
      "0.2949842709131861 0.5260815046469209 0.03811236090880852 101\n",
      "0.29504769249317986 0.5259667525332089 0.03809312282826499 102\n",
      "0.29510780915580787 0.5258579801862169 0.038074899826042945 103\n",
      "0.2951647931212952 0.5257548759985254 0.038057637729557885 104\n",
      "0.29521880763542024 0.5256571446006703 0.03804128531875122 105\n",
      "0.29527000743717524 0.5255645060149776 0.03802579415908074 106\n",
      "0.2953185392020575 0.5254766948534935 0.038011118444548776 107\n",
      "0.29536454196226075 0.5253934595577089 0.037997214850110025 108\n",
      "0.29540814750496974 0.5253145616779035 0.03798404239284802 109\n",
      "0.2954494807499 0.5252397751900426 0.03797156230135424 110\n",
      "0.29548866010716346 0.5251688858482713 0.03795973789278402 111\n",
      "0.295525797816486 0.5251016905711495 0.03794853445710067 112\n",
      "0.2955610002687481 0.5250379968598715 0.03793791914805452 113\n",
      "0.2955943683107698 0.5249776222468013 0.037927860880473625 114\n",
      "0.2956259975342135 0.5249203937727459 0.03791833023347443 115\n",
      "0.2956559785494315 0.5248661474914669 0.0379092993592248 116\n",
      "0.29568439724504403 0.5248147280000132 0.037900741896920165 117\n",
      "0.2957113350339895 0.5247659879935276 0.03789263289165281 118\n",
      "0.2957368690867542 0.5247197878432528 0.03788494871787926 119\n",
      "0.29576107255244716 0.5246759951965275 0.03787766700720723 120\n",
      "0.2957840147683559 0.5246344845976268 0.037870766580244346 121\n",
      "0.2958057614585813 0.524595137128361 0.03786422738226616 122\n",
      "0.2958263749223215 0.5245578400674025 0.03785803042247739 123\n",
      "0.29584591421234524 0.5245224865673654 0.03785215771665478 124\n",
      "0.2958644353041639 0.5244889753487132 0.0378465922329731 125\n",
      "0.2958819912563882 0.5244572104096157 0.03784131784082899 126\n",
      "0.2958986323627291 0.5244271007509268 0.037836319262488746 127\n",
      "0.2959144062960772 0.5243985601154929 0.037831582027396386 128\n",
      "0.29592935824507455 0.5243715067410464 0.037827092428989836 129\n",
      "0.29594353104356985 0.5243458631259755 0.03782283748388118 130\n",
      "0.2959569652933271 0.5243215558073006 0.037818804893266056 131\n",
      "0.2959696994803403 0.5242985151502195 0.0378149830064365 132\n",
      "0.295981770085087 0.5242766751486208 0.037811360786277226 133\n",
      "0.2959932116870361 0.5242559732359904 0.03780792777663438 134\n",
      "0.29600405706371014 0.5242363501061744 0.037804674071451314 135\n",
      "0.2960143372845853 0.52421774954348 0.0378015902855725 136\n",
      "0.2960240818000981 0.5242001182616309 0.03779866752712293 137\n",
      "0.2960333185260139 0.5241834057511143 0.03779589737137498 138\n",
      "0.2960420739233996 0.5241675641344834 0.03779327183602058 139\n",
      "0.2960503730744276 0.5241525480291991 0.03779078335777112 140\n",
      "0.2960582397542313 0.52413831441762 0.037788424770211625 141\n",
      "0.2960656964990144 0.5241248225237669 0.03778618928284057 142\n",
      "0.2960727646706126 0.5241120336965087 0.037784070461230355 143\n",
      "0.2960794645176901 0.5240999112988365 0.037782062208246855 144\n",
      "0.29608581523374694 0.5240884206029065 0.0377801587462706 145\n",
      "0.2960918350121043 0.5240775286905536 0.03777835460036517 146\n",
      "0.29609754109802383 0.5240672043589875 0.03777664458234048 147\n",
      "0.2961029498381119 0.524057418031405 0.03777502377566412 148\n",
      "0.2961080767271478 0.5240481416722589 0.037773487521172794 149\n",
      "0.29611293645247366 0.5240393487069424 0.0377720314035426 150\n",
      "0.2961175429360698 0.5240310139456594 0.03777065123847596 151\n",
      "0.2961219093744379 0.524023113511262 0.0377693430605668 152\n",
      "0.296126048276406 0.5240156247708468 0.03776810311180863 153\n",
      "0.29612997149896353 0.5240085262709184 0.037766927830709174 154\n",
      "0.2961336902812287 0.5240017976759291 0.03776581384198088 155\n",
      "0.2961372152766457 0.5239954197100227 0.03776475794677518 156\n",
      "0.2961405565835049 0.5239893741018136 0.03776375711343248 157\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and desired error threshold, \n",
    "# the function returns theta0 and theta1 when it reaches the error threshold.\n",
    "# The convergence is reached when the abs(newError - oldError) is less than the threshold.\n",
    " \n",
    "def gd2(obsX, obsY, alpha, threshold):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent L2 loss algorithm\n",
    "    Return: Iterations and L2 Error\n",
    "    \"\"\"\n",
    "    theta0 = 0\n",
    "    theta1 = 1\n",
    "    oldError = 10\n",
    "    n = len(obsX)\n",
    "    count = 0\n",
    "    while(True):\n",
    "        sum0 = 0\n",
    "        sum1 = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            sum0 += obsX[i] * theta1 + theta0 - obsY[i]\n",
    "            sum1 += (obsX[i] * theta1 + theta0 -obsY[i]) * obsX[i]               \n",
    "        theta0 = theta0 - alpha * sum0\n",
    "        theta1 = theta1 - alpha * sum1\n",
    "        newError = sqerror(obsX, obsY, theta1,theta0)        \n",
    "        tolerance = abs(newError - oldError)\n",
    "        if(tolerance <= threshold):\n",
    "            break;\n",
    "        oldError = newError\n",
    "        count += 1\n",
    "        print(theta0, theta1, newError, count)\n",
    "    return theta0, theta1, newError, count\n",
    "\n",
    "[theta0,theta1,newError,iterations] = gd2(x,y,0.01,0.000001)\n",
    "print(iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29614372377387144, 0.5239836435320433)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe theta0 and theta1\n",
    "\n",
    "theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2 Compute Gradient Descent (Huber)\n",
    "First Compute a formula for Pseudo huber gradient descent using derivative methods discussed in class and recitation. Similar to L2 descent, use the new formulas (obtained from pseudo huber derivatives) to compute values of theta1, theta1, error. The pseudo huber loss function is provided in Activity 2.3. Use that to differentiate the huber function wrt to theta0 and theta1. \n",
    "\n",
    "Please print out the theta0 and theta1 values for each iteration in your function. You may get different output compared with the sample output depending on your initial theta0 and theta1 values. We will accept any answers which are close to the sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.17562686567164243 0.345709594882729 0.003259348936712096 1\n",
      "0.5207103671641793 0.7262591706883951 0.0041851670419280574 2\n",
      "0.11281502976245161 0.49674211943233154 0.001393523283773101 3\n",
      "0.3560465577325465 0.6273046456446744 0.0023696139056498173 4\n",
      "0.2151071017351895 0.5457438100121484 0.0014457237852816493 5\n",
      "0.30061928061135595 0.5895300510083368 0.0018706638197379755 6\n",
      "0.25244663658131905 0.5596119981309464 0.0015534510969993178 7\n",
      "0.2830029971314087 0.5733777961976849 0.0017104260885184804 8\n",
      "0.26701349966344134 0.5615811729954964 0.0015956171765556318 9\n",
      "0.27837068578569857 0.5650528260035304 0.001645769602706534 10\n",
      "0.2734984868693493 0.5597009521744442 0.0016009584533003287 11\n",
      "0.2781031624828447 0.5597253548977722 0.0016130188052445634 12\n",
      "0.2770282009188542 0.5567420621406707 0.0015927176335792167 13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and desired error, \n",
    "# the function returns theta0, theta1, error and iterations\n",
    "# that reaches a minimum error threshold\n",
    "\n",
    "def gdh(obsX, obsY, alpha, threshold, delta):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent huber loss algorithm\n",
    "    Return: Iterations and huber Error\n",
    "    \"\"\"\n",
    "    theta0 = 1\n",
    "    theta1 = 1\n",
    "    oldError = 10\n",
    "    n = len(obsX)\n",
    "    count = 0\n",
    "    while(True):\n",
    "        sum0 = 0\n",
    "        sum1 = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            sum0 += obsX[i] * theta1 + theta0 - obsY[i]\n",
    "            sum1 += (obsX[i] * theta1 + theta0 -obsY[i]) * obsX[i]               \n",
    "        theta0 = theta0 - alpha * sum0\n",
    "        theta1 = theta1 - alpha * sum1\n",
    "        newError = huberror(obsX, obsY, theta1, theta0, delta)\n",
    "        \n",
    "        if(abs(newError - oldError) <= threshold):\n",
    "            break;\n",
    "        oldError = newError\n",
    "        count += 1\n",
    "        print(theta0, theta1, oldError, count)\n",
    "    return theta0, theta1, oldError, count\n",
    " \n",
    "# testing    \n",
    "[theta0,theta1,newError,iterations] = gdh(x,y,0.01,0.000001,0.01)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2 Analysis\n",
    "1. Write the values of theta0, theta1, alpha, error that provided the minimum value through gradient descent\n",
    "2. Experiment the new values of theta0, theta1 to see if the interactive widget shows similar things.\n",
    "\n",
    "##### BEGIN ANSWER\n",
    "1. - intital theta0 : 0\n",
    "   - intital theta1: 1\n",
    "   - theta0 : 0.2933359129781111\n",
    "   - theta1: 0.5290639683008116\n",
    "   - alpha: 0.01\n",
    "   - error: 0.0014825998280743565\n",
    "   - iterations: 85\n",
    "2. - New experiment:\n",
    "   - intital theta0 : 1\n",
    "   - intital theta1: 1\n",
    "   - theta0 : 0.2770282009188542\n",
    "   - theta1: 0.5567420621406707\n",
    "   - alpha: 0.01\n",
    "   - error: 0.0015927176335792167 \n",
    "   - iterations:13. \n",
    "  - With these new values of theta0, theta1, the final error is similar but with lesser of iterations.\n",
    "\n",
    "##### END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.3 Compare with Library Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29620134]\n",
      "[[0.5238794]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "x = x.reshape(-1,1)\n",
    "y = y.reshape(-1,1)\n",
    "result = lm.fit(x,y)\n",
    "print(result.intercept_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02342046]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0 = result.intercept_\n",
    "theta1 = result.coef_\n",
    "sqerror(x, y, theta0,theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4- Predict the Final Exam Score\n",
    "The regression line was obtained using grades from CS 205 course. We can consider them to be training data. Now we trained a model (with theta0 and theta1) so we can predict the grade for your own course based on your midterm grade.\n",
    "We will do few things before we can accomplish this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4.1 Read the midterm grades\n",
    "The grade file for CS439 midterm is given in data/CS439_grades.csv. Read this data file to a new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97 entries, 0 to 96\n",
      "Data columns (total 1 columns):\n",
      "midterm    97 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 856.0 bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 96 entries, 0 to 96\n",
      "Data columns (total 1 columns):\n",
      "midterm    96 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 1.5 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_439 = pd.read_csv(\"data/CS439_grades_03_15_19.csv\")\n",
    "df_439.info()\n",
    "mid = df_439[df_439['midterm']<80]\n",
    "mid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4.2 Predict your Grade\n",
    "Scale the values in the midterm grades of CS 439 and compute the estimated final exam grade. Note that this is probably not a very good estimator since we are trying to predict final exam just by using a midterm score. However, using more features such as labs and quiz scores can help improve the accuracy. We will do that in a future lab. The output is shown as values scaled back to percentages (100% max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midterm, final_pred\n",
      "[[58.5  61.9 ]\n",
      " [51.   51.8 ]\n",
      " [55.   57.2 ]\n",
      " [39.5  36.3 ]\n",
      " [71.5  79.3 ]\n",
      " [56.5  59.2 ]\n",
      " [50.   50.4 ]\n",
      " [52.5  53.8 ]\n",
      " [51.5  52.5 ]\n",
      " [43.   41.  ]\n",
      " [53.5  55.1 ]\n",
      " [51.5  52.5 ]\n",
      " [64.   69.2 ]\n",
      " [60.   63.9 ]\n",
      " [55.   57.2 ]\n",
      " [51.5  52.5 ]\n",
      " [58.5  61.9 ]\n",
      " [51.   51.8 ]\n",
      " [58.   61.2 ]\n",
      " [73.5  82.  ]\n",
      " [62.   66.6 ]\n",
      " [58.5  61.9 ]\n",
      " [34.5  29.6 ]\n",
      " [55.5  57.8 ]\n",
      " [48.   47.8 ]\n",
      " [67.   73.3 ]\n",
      " [61.   65.2 ]\n",
      " [70.   77.3 ]\n",
      " [46.   45.1 ]\n",
      " [63.   67.9 ]\n",
      " [61.   65.2 ]\n",
      " [35.5  31.  ]\n",
      " [58.5  61.9 ]\n",
      " [61.5  65.9 ]\n",
      " [53.5  55.1 ]\n",
      " [73.   81.3 ]\n",
      " [48.   47.8 ]\n",
      " [58.   61.2 ]\n",
      " [52.   53.1 ]\n",
      " [54.   55.8 ]\n",
      " [61.5  65.9 ]\n",
      " [58.   61.2 ]\n",
      " [67.   73.3 ]\n",
      " [45.   43.7 ]\n",
      " [58.5  61.9 ]\n",
      " [42.5  40.4 ]\n",
      " [42.   39.7 ]\n",
      " [37.   33.  ]\n",
      " [66.   71.9 ]\n",
      " [52.   53.1 ]\n",
      " [45.   43.7 ]\n",
      " [40.   37.  ]\n",
      " [61.   65.2 ]\n",
      " [54.5  56.5 ]\n",
      " [44.   42.4 ]\n",
      " [65.   70.6 ]\n",
      " [43.5  41.7 ]\n",
      " [58.   61.2 ]\n",
      " [62.5  67.2 ]\n",
      " [68.   74.6 ]\n",
      " [54.   55.8 ]\n",
      " [49.   49.1 ]\n",
      " [63.   67.9 ]\n",
      " [59.   62.5 ]\n",
      " [57.5  60.5 ]\n",
      " [62.   66.6 ]\n",
      " [36.5  32.3 ]\n",
      " [48.   47.8 ]\n",
      " [42.   39.7 ]\n",
      " [54.   55.8 ]\n",
      " [61.   65.2 ]\n",
      " [37.5  33.6 ]\n",
      " [61.   65.2 ]\n",
      " [61.   65.2 ]\n",
      " [55.5  57.8 ]\n",
      " [50.   50.4 ]\n",
      " [51.5  52.5 ]\n",
      " [73.   81.3 ]\n",
      " [65.5  71.3 ]\n",
      " [69.   76.  ]\n",
      " [56.   58.5 ]\n",
      " [60.   63.9 ]\n",
      " [57.   59.8 ]\n",
      " [52.5  53.8 ]\n",
      " [54.75 56.8 ]\n",
      " [62.   66.6 ]\n",
      " [54.5  56.5 ]\n",
      " [51.5  52.5 ]\n",
      " [37.   33.  ]\n",
      " [46.5  45.7 ]\n",
      " [72.   80.  ]\n",
      " [61.5  65.9 ]\n",
      " [59.5  63.2 ]\n",
      " [49.5  49.8 ]\n",
      " [51.   51.8 ]\n",
      " [67.   73.3 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "m = scaler.fit_transform(mid)\n",
    "f_pred = (result.predict(m))*100\n",
    "final_pred = np.around(f_pred, 1)\n",
    "print('midterm, final_pred')\n",
    "print(np.c_[mid,final_pred] )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Developed by A.D. Gunawardena @ 2019 for CS 439"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
